DRBD 0.8 Roadmap
----------------

1 Drop support for linux-2.4.x. 
  Do all size calculations on the base of sectors (512 Byte) as it 
  is common in Linux-2.6.x.
  (Currently they are done on a 1k base, for 2.4.x compatibility)

2 Drop the Drbd_Parameter_Packet.
  Replace the Drbd_Parameter_Packet by a more general and 
  extensible mechanism.

3 Authenticate the peer upon connect by using a shared secret. 
  Configuration file syntax:  net { auth-secret "secret-word" }
  Using a challenge-response authentication within the new
  handshake.

4 Changes of state and cstate synchronized by mutex and only done by
  the worker thread.

5 Two new configuration options, to allow more fine grained definition of
  DRDBs behaviour after a split-brain situation:

  after-sb-2pri = 
   disconnect     No automatic resynchronisation gets performed. One
                  node should drop its net-conf (preferable the
                  node that would become sync-target)
                  DEFAULT.
   discard-younger-primary
                  Auto sync from is the oder primary (curr.behaviour i.t.s.)
   discard-older-primary
                  Auto sync from is the younger primary
   discard-less-modified
                  Auto sync from is the node that did more modifications
   discard-NODENAME 
                  Auto sync to the named node 
  
  pri-sees-sec-with-higher-gc =
   disconnect         (current behaviour)
   discard-secondary  Auto sync from is the current primary
   suspend_io          The current primary freezes IO.

  pri-sees-sec-with-higher-gc-cmd "command";
                      In the same event this command will be executed.
  
  
  Notes:
  1) The disconnect actions cause the sync-target or the secondary
     (better both) node to go into StandAlone state.
  2) If two nodes in primary state try to connect one (better both)
     of them goes into StandAlone state (=curr. behaviour)
  3) As soon as the decision is takes the sync-target adopts the
     GC of the sync source. 
     [ The whole algorithm would also work if both would reset their 
       GCs to <0,0,0...> after the decision, but since we also
       use the GC to tag the bitmap it is better the current way ]
  4) The execution of the pri-sees-sec-with-higher-gc-cmd should
     be implemented like the kernel can execute modprobe...

6 It is possible that a secondary node crashes a primary by 
  returning invalid block_ids in ACK packets. [This might be 
  either caused by faulty hardware, or by a hostile modification
  of DRBD on the secondary node]

  Proposed solution:

  Have a hash table (hlist_head style), add the collision
  member (hlist_node) to drbd_request. 

  Use the sector number of the drbd_request as key to the hash, each
  drbd_request is also put into this hash table. We still use the 
  pointer as block_id. 

  When we get an ACK packet, we lookup the hash table with the
  block_id, and may find the drbd_request there. Otherwise it 
  was a forged ACK.

  Note: The actual key to the hash should be (sector & ~0x7).
        See item 9 for more details.

7 Handle split brain situations; Support IO fencing; 
  introduce the "Dead" peer state (o_state)

  New commands:
    drbdadm resume r0
    drbdadm outdate r0 
    drbdadm suspend r0

  remove option value: on-disconnect=suspend_io

  introduce: 
    peer-state-unknown=suspend_io
    peer-state-unknown=continue_io

  New meta-data flag: "Outdated"

  Let us assume that we have two boxes (N1 and N2) and that these
  two boxes are connected by two networks (net and cnet [ clinets'-net ]).

  Net is used by DRBD, while heartbeat uses both, net and cnet

  I know that you are talking about fencing by STONITH, but DRBD is
  not limited to that. Here comes my understanding of how fencing
  (other than STONITH) should work with DRBD-0.8 :

   N1  net   N2
   P/S ---  S/P     everything up and running.
   P/? - -  S/?     network breaks ; N1 freezes IO
   P/? - -  S/?     N1 fences N2:
                    In the STONITH case: turn off N2.
                    In the "smart" case: 
                    N1 asks N2 to fence itself from the storage via cnet.
                    HB calls "drbdadm outdate r0" on N2.
                    N2 replies to N1 that fencing is done via cnet.
                    N1 calls "drbdadm resume r0".
   P/D - -  S/?     N1 thaws IO

  N2 got the the "Outdated" flag set in its meta-data, by the outdate 
  command. The suspend command is here to make the interface 
  complete, and to reach the freezed state without th need to
  disconnect the peer. It might turn out to be usefull for other
  tasks as well.

  Eventually introduce on-disconnent-cmd "command";

8 New command drbdmeta

  We move the read_gc.pl/write_gc.pl to the user directory. 
  Make them to one C program: drbdmeta
   -> in the future the module never creates the meta data
      block. One can use drbdmeta to create, read and 
      modify the drbdmeta block. drbdmeta refuses to write
      to it as long as the module is loaded (configured).

  drbdsetup gets the ability to read the gc values while DRBD
  is set up via an ioctl() call. -- drbdmeta refuses to run
  if DRBD is configured. 

  drbdadm is the nice front end. It always uses the right 
  back end (drbdmeta or drbdsetup)...

  drbdadm md-set-gc 1:2:3:4:5:6 r0
  drbdadm md-get-gc r0
  drbdadm md-get/set-{la-size|consistent|etc...} resources....
  drbdadm md-create r0

  md-create would ask nasty questions about whether you are really 
  sure and so on, and do some plausibility checks first.
  md-set would be undocumented and for wizards only.

9 Support shared disk semantics  ( for GFS, OCFS etc... )

    All the thoughts in this area, imply that the cluster deals
    with split brain situations as discussed in item 6.

  In order to offer a shared disk mode for GFS, we allow both
  nodes to become primary. (This needs to be enabled with the
  config statement net { allow-two-primaries; } )

 Read after write dependencies

  The shared state is available to clusters using protocol C
  and B. It is not usable with protocol A.

  To support the shared state with protocol B, upon a read
  request the node has to check if a new version of the block
  is in the progress of getting written. (== search for it on
  active_ee and done_ee. [ Since it is on active_ee before the 
  RecvAck is sent. ] )
  
 Global write order

  The major pitfall is the handling of concurrent writes to the
  same block. (Concurrent writes to the same blocks should not 
  happen, but we have to assume that it is possible that the
  synchronisation methods of our upper layer [i.e. openGFS] 
  may fail.)

  Without further handling concurrent writes to the same block
  would get written on each node locally first, then sent
  to the peer and then overwrite the local version on the peer.
  In other words, each node would write its local version first,
  and the peers version of the data.

  Both nodes need to agree to _one_ order, in which such 
  conflicting writes should be carried out.

  Proposed Solution

  We arbitrary select one node (e.g. the node that did the first
  accept() in the drbd_connect() function) and mark it withe the
  discard-concurrent-write-flag.

  The algorithm which is performed upon the reception of a 
  data packet.

  1. Do we have a concurrent request? (i.e. Do I have a request
     to the same block in my transfer log.) If not -> write now.
  2. Have I already got an ACK packet for the concurrent 
     request ? (Has the request the RQ_DRBD_SENT bit already set)
     If yes -> write the data from the data packet afterwards.
  3. Do I have the "discard-concurrent-write-flag" ?
     If yes -> discard the data packet and send an discard notify.
     If no -> Write data from the data packet afterwards.

  BTW, each time we have a concurrent write access, we print
  a warning to the syslog, since this indicates that the layer
  above us is broken!

  Note: In Item 6 we created a hash table over all requests in the
        transfer log, keyed with (sector & ~0x7). This allows us
        to find IO operations starting in the same 4k block of
        data quickly. -> With two lookups the hash table we can
	find any concurrent access.

  [ see also GFS-mode-arbitration.pdf for illustration. ]

10 Change Sync-groups to sync-after
  
  Sync groups turned out to be hard to configure and more
  complex setups, hard to implement right and last not least they
  are not flexible enough to cover all real world scenarios.

  E.g. Two physical disks should be mirrored with DRBD. On one
       of the disks there is only a single partition, while the
       other one is divided into many (e.g. 4 smaller) partitions.
       One would want to sync the big one in parallel to the 
       4 small ones. While the resync process of the 4 small
       ones need to be serialized. 
       -> With the current sync groups you can not express
          this requirement.

  Remove config options   syncer { group <number>; }
  Introduce config options   syncer { after <resource>; }
  
11 Take into account that the two systems could have different 
  PAGE_SIZE. 

  At least we should negotiate the PAGE_SIZE used by the peers,
  and use it. In case the PAGE_SIZE is not the same inform
  the user about the fact.

  Probably a general high performance implementation for this
  issue is not necessary, since clusters of machines with 
  different PAGE_SIZE are of academic interest only.
 

plus-banches:
----------------------

1 wait-sync-target  

2 Implement the checksum based resync. 

3 Have protocol version 74 available in drbd-0.8, to allow rolling 
  upgrades

4 Change the bitmap code to work with unmapped highmem pages, instead
  of using vmalloc()ed memory. This allows users of 32bit platforms
  to use drbd on big devices (in the ~3TB range)

5 Support for variable sized meta data (esp bitmap) = Support for more
  than 4TB of storage.

6 3 node support. Do and test a 3 node setup (2nd DRBD stacked over
  a DRBD pair). Enhance the user level tools to support the 3 node
  setup.

7 Support to pass LockFS calls / make taking of snapshots possible (?)
