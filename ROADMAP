DRBD 0.8 Roadmap
----------------

1 Drop support for linux-2.4.x. 
  Do all size calculations on the base of sectors (512 Byte) as it 
  is common in Linux-2.6.x.
  (Currently they are done on a 1k base, for 2.4.x compatibility)
  90% DONE

2 Drop the Drbd_Parameter_Packet.
  Replace the Drbd_Parameter_Packet by 4 small packets:
  Protocol, GenCnt, Sizes and State.
  The receiving code of these small packets is sane, compared
  to that huge receive_params() function we had before.
  40% DONE

3 Authenticate the peer upon connect by using a shared secret. 
  Configuration file syntax:  net { cram-hmac-alg "sha1"; 
  shared-secret "secret-word"; }
  Using a challenge-response authentication.
  99% DONE

4 Consolidate state changes into a central function, that makes
  sure that the new state is valid. Replace set_cstate() with
  a force_state() and a request_state() function. Make all
  state changes atomic, and consolidate the many differenct 
  cstate-error states into a single "NetworkFailure" state.
  50% DONE

5 Three configuration options, to allow more fine grained definition
  of DRBDs behaviour after a split-brain situation:

  In case the nodes of your cluster nodes see each other again, after
  an split brain situation in which both nodes where primary 
  at the same time, you have two diverged versions of your data.
 
  In case both nodes are secondary you can control DRBD's
  auto recovery strategy by the "after-sb-0pri" options. The
  default is to disconnect. 
     "disconnect" ... No automatic resynchronisation, simply disconnect.
     "discard-younger-primary"
                      Auto sync from the node that was primary before
                      the split brain situation happened.
     "discard-older-primary"
                      Auto sync from the node that became primary 
                      as second during the split brain situation.
     "discard-least-changes"
                      Auto sync from the node that touched more
                      blocks during the split brain situation.
     "discard-node-NODENAME"
                      Auto sync _to_ the named node.

  In one of the nodes is already primary, then the auto-recovery
  strategie is controled by the "after-sb-1pri" options.
     "disconnect" ... always disconnect 
     "consensus"  ... discard the version of the secondary if the outcome
                      of the "after-sb-0pri" algorithm would also destroy 
                      the current secondary's data. Otherwise disconnect.
     "discard-secondary"     
                      discard the version of the secondary.
     "panic-primary"  Always honour the outcome of the "after-sb-2sc"
                      algorithm. In case it decides the the current
                      secondary has the right data, it tries to make the
                      the current primary secondary, if that fails 
                      it panics the current primary.

  In case both nodes are primary you control DRBD's strategy by
  the "after-sb-2pri" option.
     "disconnect" ... Go to StandAlone mode on both sides.
     "panic"      ... Honor the outcome of the "after-sb-0pri" algorithm
                      and panic the other node.

  Defaults:
  after-sb-0pri = disconnect;
  after-sb-1pri = disconnect;
  after-sb-2pri = disconnect;

  DRBD-07 was:
  after-sb-0pri = discard-younger-primary;
  after-sb-1pri = consensus;
  after-sb-2pri = disconnect;

  NB: To allow the user to resolve from such situations manually
      the "drbdadm connect" command (this is the "drbdsetup net"
      command) gets a short-living flag called "--I-want-to-be-discarded".

6 It is possible that a secondary node crashes a primary by 
  returning invalid block_ids in ACK packets. [This might be 
  either caused by faulty hardware, or by a hostile modification
  of DRBD on the secondary node]

  Proposed solution:

  Have a hash table (hlist_head style), add the collision
  member (hlist_node) to drbd_request. 

  Use the sector number of the drbd_request as key to the hash, each
  drbd_request is also put into this hash table. We still use the 
  pointer as block_id. 

  When we get an ACK packet, we lookup the hash table with the
  block_id, and may find the drbd_request there. Otherwise it 
  was a forged ACK.

  Note: The actual key to the hash should be (sector & ~0x7).
        See item 9 for more details.
  80% DONE ; There are more places where we case an block_id
             to an req pointer, need to make sure that we
             cover all of these places.

7 Handle split brain situations; Support IO fencing; 
  
  New commands:
    drbdadm outdate r0

    When the device is configured this works via an ioctl() call.
    In the other case it modifies the meta data directly by 
    calling drbdmeta.

  remove option value: on-disconnect=suspend_io

  New meta-data flag: "Outdated"

  introduce:
  disk {
    split-brain-fix;
  }

  handlers {
    outdate-peer "some script";
  }

  If the disk state of the peer is unknown, drbd calls this 
  handler (yes a call to userspace from kernel space). The handler's
  returncodes are:

  3 -> peer is inconsistent
  4 -> peer is outdated (presumabely this handler outdated it)
  5 -> peer was down / unreachable
  6 -> peer is primary

  Let us assume that we have two boxes (N1 and N2) and that these
  two boxes are connected by two networks (net and cnet [ clinets'-net ]).

  Net is used by DRBD, while heartbeat uses both, net and cnet

  I know that you are talking about fencing by STONITH, but DRBD is
  not limited to that. Here comes my understanding of how fencing
  (other than STONITH) should work with DRBD-0.8 :

   N1  net   N2
   P/S ---  S/P     everything up and running.
   P/? - -  S/?     network breaks ; N1 freezes IO
   P/? - -  S/?     N1 fences N2:
                    In the STONITH case: turn off N2.
                    In the "smart" case: 
                    N1 asks N2 to fence itself from the storage via cnet.
                    HB calls "drbdadm outdate r0" on N2.
                    N2 replies to N1 that fencing is done via cnet.
                    The outdate-peer script on N1 returns sucess to DRBD.
   P/D - -  S/?     N1 thaws IO

  N2 got the the "Outdated" flag set in its meta-data, by the outdate 
  command. 

  The "split-brain-fix" enables this behaviour. If this option is
  omitted, the handler is not called nor IO is frozen on disconnect.

  Eventually introduce a "suspend" and a "resume" command to 
  to reach the freezed state without the need to disconnect the peer. 
  It might turn out to be usefull for other tasks as well.

  66% DONE / TODO: IO freezing is not done yet.

8 New command drbdmeta

  We move the read_gc.pl/write_gc.pl to the user directory. 
  Make them to one C program: drbdmeta
   -> in the future the module never creates the meta data
      block. One can use drbdmeta to create, read and 
      modify the drbdmeta block. drbdmeta refuses to write
      to it as long as the module is loaded (configured).

  drbdsetup gets the ability to read the gc values while DRBD
  is set up via an ioctl() call. -- drbdmeta refuses to run
  if DRBD is configured. 

  drbdadm is the nice front end. It always uses the right 
  back end (drbdmeta or drbdsetup)...

  drbdadm set-gi 1:2:3:4:5:6 r0
  drbdadm get-gi r0
  drbdadm md-create r0

  md-create would ask nasty questions about whether you are really 
  sure and so on, and do some plausibility checks first.
  md-set would be undocumented and for wizards only.
  80% DONE

9 Support shared disk semantics  ( for GFS, OCFS etc... )

    All the thoughts in this area, imply that the cluster deals
    with split brain situations as discussed in item 6.

  In order to offer a shared disk mode for GFS, we allow both
  nodes to become primary. (This needs to be enabled with the
  config statement net { allow-two-primaries; } )

 Read after write dependencies

  The shared state is available to clusters using protocol C
  and B. It is not usable with protocol A.

  To support the shared state with protocol B, upon a read
  request the node has to check if a new version of the block
  is in the progress of getting written. (== search for it on
  active_ee and done_ee. [ Since it is on active_ee before the 
  RecvAck is sent. ] )
  
 Global write order

  [ Description of GFS-mode-arbitration2.pdf ]

  1. Basic mirroring with protocol C.
    The file system on N2 issues a write request towards DRBD, 
    which is written to the local disk and sent to N1. Then
    the data bock is written to the local disk here and and
    acknowledge packet is sent back. As soon as both the
    write to the local disk and the ACK from N1 reach N2, 
    DRBD signals the completion of IO to the file system.

    The major pitfall is the handling of concurrent writes to the
    same block. (Concurrent writes to the same blocks should not 
    happen, but we have to assume that it is possible that the
    synchronisation methods of our upper layer [i.e. openGFS] 
    may fail.)

    There are many cases in which such concurrent writes would
    lead to different data on our two copies of the block. 

  2. Concurrent writes, network latency is lower than disk latency
    As we can see on the left side in figure two this could lead
    to N1 has the blue version (=data from FS on N2) while N2
    ends with having the green version (=data from FS on N1).
    The solution is to flag one node (in the example N2 has the
    discard-concurrent-writes-flag).
    As we can see on the right side, now both nodes ends with 
    the blue data.

  3. Concurrent writes, high latency for data packets.
    The problem now is that N2 does can not detect that this was
    a concurrent write, since it got the ACK before the conflicting
    data packets comes in. 
    This can happens since in DRBD, data packets and ACK packets are
    transmitted via two independent TCP connections, therefore the
    ACK packet can overtakes a data packet.
    The solution is to send with the ACK packet a discard info packet,
    which identifies the data packet by it sequence number.
    N2 will keep this discard info as long as it has not seen higher
    sequence numbers by now.
    With this both nodes will end with the blue data.

  4. Concurrent writes, high latency for data packets.
    This is the inverse case to case3 and already handled by the means
    introduced with item 1. 

  5. New write while processing a write from the peer.
    Without further measures this would lead to an inconsistency in 
    our mirror as the figure on the left side shows. 
    If we currently write a conflicting block from the peer, we simply
    discard the write request from our FS and signal IO completion 
    immediately.

  6. High disk latency on N2.
    By IO reordering in the layers below us this could lead to 
    having the blue data on N2 and the green data on N1. 
    The solution to this case is the delay the write to the local
    disk on N2 until the local write is done. This is different from
    case two since we already got the write ACK to the conflicting
    block.

  7. An data packet overtakes an ACK packet on the network.
    Although this case is quite unlikely, we have to take int into 
    account. From N2's point of fiew this looks a lot like case 4,
    but N2 should not delete the data packet now!

 Proposed solution

  We arbitrary select one node (e.g. the node that did the first
  accept() in the drbd_connect() function) and mark it withe the
  discard-concurrent-writes-flag.

  Each data packet and each ACK packet gets a sequence 
  number, which is increased which every packet sent. 
  (This is a common space of sequence numbers)

  The algorithm which is performed upon the reception of a 
  data packet [drbd_receiver].

  *  If the sequence number of the data packet is higher than
     last_seq+1 sleep until last_seq+1 == seq_num(data packet)
     [needed to satisfy example case 7]

  1. If the packet's sequence number is on the discard list,
     simply drop it. 
     [ ex.c. 3]
  2. Do we have a concurrent request? (i.e. Do I have a request
     to the same block in my transfer log.) If not -> write now.
     [ default ]
  3. Have I already got an ACK packet for the concurrent 
     request ? (Has the request the RQ_DRBD_SENT bit already set)
     If yes -> write the data from the data packet afterwards.
     [ ex.c. 6]
  4. Do I have the "discard-concurrent-write-flag" ?
     If yes -> discard the data packet.
     If no -> Write data from the data packet afterwards and set
              the RQ_DRBD_SENT bit in the request object ( Since
              will will not get an ACK from our peer). Mark the
	      ee to prepend the ACK packet with a discard info
	      packet.
     [ ex.c. *]

  The algorithm which is performed upon the reception of an 
  ACK packet [drbd_asender]

  * If we get an ACK, store the sequence number in last_seq.

  The algorithm which is performed upon the reception of an 
  discard info packet [drbd_asender]

  * if the current last_seq is lower the the packet that should
    be discarded, store it in the to discard list.

  BTW, each time we have a concurrent write access, we print
  a warning to the syslog, since this indicates that the layer
  above us is broken!

  Note: In Item 6 we created a hash table over all requests in the
        transfer log, keyed with (sector & ~0x7). This allows us
        to find IO operations starting in the same 4k block of
        data quickly. -> With two lookups the hash table we can
	find any concurrent access.
  99% DONE

10 Change Sync-groups to sync-after
  
  Sync groups turned out to be hard to configure and more
  complex setups, hard to implement right and last not least they
  are not flexible enough to cover all real world scenarios.

  E.g. Two physical disks should be mirrored with DRBD. On one
       of the disks there is only a single partition, while the
       other one is divided into many (e.g. 4 smaller) partitions.
       One would want to sync the big one in parallel to the 
       4 small ones. While the resync process of the 4 small
       ones need to be serialized. 
       -> With the current sync groups you can not express
          this requirement.

  Remove config options   syncer { group <number>; }
  Introduce config options   syncer { after <resource>; }
  0% DONE
  
11 Take into account that the two systems could have different 
  PAGE_SIZE. 

  At least we should negotiate the PAGE_SIZE used by the peers,
  and use it. In case the PAGE_SIZE is not the same inform
  the user about the fact.

  Probably a general high performance implementation for this
  issue is not necessary, since clusters of machines with 
  different PAGE_SIZE are of academic interest only.
  100% DONE by item 15

12 Introduce a "common" section in the config file. Option
  section (like handlers, startup, disk, net and syncer)
  are inherited from the common section, if they are not
  defined in a resource section.
  99% DONE

13 Introduce an UUID (universally unique identifier) in the
  meta data. One purpose is to tag the bitmap with this UUID. 
  If the peer's UUID is different to what we expect we know that 
  we have to do a full sync....
  99% DONE
  -> Will be go out again, and become replaced by UUID for data
     generations. See item 16

14 Sanitize ioctls to inlcude a standard device information struct
  at the beginning, including the expected API version.
  Consider using DRBD ioctls with some char device similar to
  /dev/mapper/control
  0% DONE

15 Accept BIOs bigger than one page, probabely up to 32k (8 pages) 
  currently. When this is done make the bits in the bitmap to account 
  for more then 4k e.g. 64k
  50% DONE we handle big BIOs now, a bitmap bit is still 4k.

16 Displace the current generation-counters with a data-generation-UUID 
   concept.
  The current generation counters have various weaknesses:
   * In a split braine'd cluster the appliance of the same events
     to both cluster nodes could lead to equal generation-counters
     on both nodes, while the data is not in sync for sure.
   * They are completely unsuitable if a 3rd node is used for 
     e.g. weekly snapshots.
   * Lots of other shortcommings.

  We associate each data generation with an unique UUID (=64 bit random
  number). A new data generation is created if a primary node is 
  disconnected from its secondary and when a degraded secondary 
  becomes primary for the first time.

  In the meta-data we store a few generations-UUIDs: 
   * current
   * bitmap
   * history[2]

  As well as the currently known flags: 
   Consistent, WasUpToDate, LastState, ConnectedInd, WantFullSync

  When the cluster is in Connected state, then the bitmpat gen-UUID
  is set to 0 (Since the Bitmap is empty). When we create a new current
  gen-UUID while we are disconencted the (old) current gets backed-up
  to the bitmap gen-UUID. (This allowes us to identify the base of
  of the bitmap later)

  Special UUID values:
  JustCreated [JC] ___  4

  ALGORITHMS

  Upon Connect:
      self   peer   action
  1.  C=JC   C=JC   No Sync
  2.  C=JC   C!=JC  I am SyncTarget setting BM
  3. C!=JC   C=JC   I am SyncSource setting BM
  4.   C   =   C    No Sync
  5.   C   =   B    I am SyncTarget using BM
  6.   C   = H1|H2  I am SyncTarget setting BM
  7.   B   =   C    I am SyncSource using BM
  8. H1|H2 =   C    I am SyncSource setting BM
  9.   B   =   B    [ and B != 0 ] SplitBrain, try auto recover strategies.
  10 H1|H2 = H1|H2  SplitBrain, disconnect.
  11.               Warn about unrelated Data, disconnect.

  Upon Disconnect:
   Primary:
      Copy the current-UUID over to the bitmap-UUID, create a new
      current-UUID.
   Secondary:
      Nothing to do.

  Upon becomming Primary:
   In case we are disconnected and the bitmap-UUID is emptry, copy the
   current-UUID over to the bitmap-UUID and create a new current-UUID.
   Special-case: primary with --do-what-I-say, clearing the inconsistent
                 flag causes a new UUID to be generated.

  Upon start of resync:
   Clear the consistent-flag on the SyncTarget. The SyncTarget sets its
   current-UUID to the peer's bitmap-UUID.

  Upon finish of resync:
   Set the bitmap-UUID to 0. The SyncTarget addopts the current-UUID
   of the SyncSource, and sets its consistent-flag.

  When the bitmap-UUID gets cleared, move the previous value to H1.
  In case H1 was already set copy its previous value to H2. Etc..

  For the auto recover strategies after split brain (see item 5) 
  it is neccessary to embedd the node's role into the UUIDs.
  This is masked out of course when the UUIDs are compared.

  * Note1: Discontinue the --human and --timout options when 
           becoming primary.
           NB: If they are needed, I think they can be implemented
               as special UUID values.

  99% DONE. Kernel part is implemented, userlang parts are implemented,
	    --humand and --timeout-expired are removed.
	    Everything seems to work so far.

17 Something like

   drbdx: WARNING disk sizes more than 10% different

  would be nice at (initial) full sync.
  drbdx: WARNING disk sizes more than 10% different   

18 Connection-Teardown Packet. Currently the new state-checks
  disallows "drbdadm disconnect res" on the primary node of a
  connected cluster.
  Thes Teardown Packet causes the secondary-node to outdate
  its data and to close the connection in one go.
  0% DONE.

19 Should we expect the user to use "set-gi" to recover from a
  aplit brain (two disconnected primaries) situation ?
  Should we provide him commands like "drbdadm winner res"
  "drbdadm looser res", to resolve the situation.

20 Make the updates to the bitmap transactional. Esp for resizing.
  Make updates to the superblock transactional

21 There are quite a number of parameters that must be set equal
   (or some reciprocal) on the two nodes. We need to ensure that
   the config is valid, from a viewpoint of the whole cluster.
   E.g.
   protocol				equal
   cram-hmac-alg			equal
   after-sb-0pri / discard-local/remote	equal / reciprocal
   after-sb-1pri			equal
   after-sb-2pri			equal
   syncer/group				equal

plus-banches:
----------------------

1 wait-sync-target  

2 Implement the checksum based resync
  and online verification.

3 Have protocol version 74 available in drbd-0.8, to allow rolling 
  upgrades

4 Change the bitmap code to work with unmapped highmem pages, instead
  of using vmalloc()ed memory. This allows users of 32bit platforms
  to use drbd on big devices (in the ~3TB range)

5 Support for variable sized meta data (esp bitmap) = Support for more
  than 4TB of storage.

6 3 node support. Do and test a 3 node setup (2nd DRBD stacked over
  a DRBD pair). Enhance the user level tools to support the 3 node
  setup.

7 Support to pass LockFS calls / make taking of snapshots possible (?)

8 Dynamic device allocation (displace minor_count module parameter)

